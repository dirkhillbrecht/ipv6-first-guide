== Setup the KVM environment

The physical host is now up and running. To actually host virtual machines with IPv6-first connectivity, some more services need to be installed.


=== NAT64 with Tayga

As I wrote, our virtual machines shall have IPv6-only internet access. That implies that they _cannot_ access systems which are IPv4-only. Unfortunately, even in 2022 there are quite popular sites like `github.com` which do not have any IPv6 connectivity at all. To make such systems accessible from the guest systems, we setup a NAT64 service which performs a network address translation for exactly this case.

I decided to go with the "Tayga" server. It's scope is limited to exactly perform NAT64. This makes it necessary to add further services to make all this really useable but it also minimizes configuration complexity.

[[sec-how-tayga-works]]
==== How Tayga works (in this setup)

Tayga acts as a network address translator.
It receives incoming IPv6 packets targetted at an address in the `64:ff9b::/96` network.
It takes the _least_ 32 bits of this address and takes them as IPv4 target address.
I.e. the IPv4 address `1.2.3.4` will be represented by the IPv6 address `64:ff9b::102:304`.

.Representation of IPv4 address parts in IPv6 addresses
[NOTE]
====
IPv4 uses decimal numbers to represent address parts, IPv6 uses hexadecimal representation.
An IPv4 address like `85.223.40.38` will look a bit different therefore in the NAT64 IPv6 representation as it reads `64:ff9b::55df:2826` in the usual IPv6 address representation.

It is allowed to write the lowest 32 bits of an IPv6 address in the usual IPv6 address syntax,
so the address can also be written as `64:ff9b::85.223.40.38`.
The bits, however, are exactly the same.

I refrain from this syntax in this guide.
====

Additionally, Tayga takes the IPv6 source address and maps it onto a private IPv4 address, e.g. one out of `192.168.255.0/24`.
With these two IPv4 addresses it constructs an IPv4 packet with the content of the received IPv6 packet and puts it into the IPv4 network stack of the physical host.

Now, Linux takes over. It uses its standard source NAT mechanisms to map the IPv4 packet with the private source address onto the public IPv4 address of the machine and sends it out.
When the answer package arrives, it converts the target address back onto the private IPv4 address the packet emerged from and forwards it to the Tayga process.

Tayga can rebuild the IPv6 origin of the communication from the private IPv4 target address of the answer packet.
It also can derive the correct `64:ff9b::/96` source address from the IPv4 source address of the packet.
With these two addresses it builds an IPv6 packet with the same content as the received IPv4 packet and sends it to the actual IPv6 communication partner.

The whole scheme looks like this:

.NAT64 communication processing
[plantuml,format="svg",align="center"]
....
@startuml
hide footbox

participant "IPv6 sender\n2001:db8::1" as s
participant "IPv6 side\nof Tayga" as t6
participant "IPv4 side\nof Tayga" as t4
participant "IPv4\nnetwork stack" as l
participant "IPv4 interface\n5.6.7.8" as n
participant "IPv4 target\n1.2.3.4" as t

== Send request ==

s->t6: Send IPv6 packet\nfrom 2001:db8::1\nto 64:ff9b::102:304
t6->t4: Construct IPv4 target address\n1.2.3.4 from 64:ff9b::102:304
t4->l: Assign private source address\n192.168.255.45 to\nIPv6 source 2001:db8::1\nand build complete IPv4 packet
l->n: Usual IPv4 source NAT\nfrom 192.168.255.45\nto 5.6.7.8
n->t: Send IPv4 packet\nfrom 5.6.7.8\nto 1.2.3.4

== Receive response ==

t->n: Send IPv4 packet\nfrom 1.2.3.4\nto 5.6.7.8
n->l: Pass packet into\nnetwork stack
l->t4: Reverse IPv4 NAT,\ndeliver packet\nto 192.168.255.45
t4->t6: Find original\nsource 2001:db8::1\nfor 192.168.255.45
t6->s: Construct IPv6\nsource 64:ff9b::102:304\nfrom 1.2.3.4\nand deliver IPv6 packet
@enduml
....


==== Installing Tayga on the physical host

Start by installing the tayga service by the usual `apt install tayga`.

In `/etc/tayga.conf`, enable the disabled `ipv6-addr` directive as this is needed for working with the well-known prefix. Set the IPv6 address to something random in your IPv6 subnet:

.Random network address for the tayga NAT64 service
----
ipv6-addr 2a01:4f8:1:3:135d:6:4b27:5f
----

Additionally, switch the `prefix` directive from the activated `2001...` one to the `64:ff9b::/96` one:

.Change Tayga's prefix
----
# prefix 2001:db8:1:ffff::/96
prefix 64:ff9b::/96
----

The whole Tayga configuration reads like this afterwards:

.Tayga configuration on the physical host
----
# Minimum working NAT64 Tayga configuration for KVM host with IPv6-only guests

# (A) Basic setup
# Device name, this is the default
tun-device nat64
# Data dir for stateful NAT information
data-dir /var/spool/tayga

# (B) IPv6 setup
# The "well-known" prefix for NAT64
prefix 64:ff9b::/96
# IPv6 address, from the official ::/64 network
ipv6-addr 2a01:4f8:X:Y:14a5:69be:7e23:89

# (C) IPv4 setup
# Pool of dynamic addresses
dynamic-pool 192.168.255.0/24
# IPv4 address, not to be used otherwise in the network
ipv4-addr 192.168.255.1
----

Test the new setup by starting `tayga` once in foreground:

----
systemctl stop tayga  <-- Disable if already started
tayga -d --nodetach
----

This should give something like this:

.Output of Tayga running in foreground
----
starting TAYGA 0.9.2
Using tun device nat64 with MTU 1500
TAYGA's IPv4 address: 192.168.255.1
TAYGA's IPv6 address: 2a01:4f8:1:3:135d:6:4b27:5f
NAT64 prefix: 64:ff9b::/96
Note: traffic between IPv6 hosts and private IPv4 addresses (i.e. to/from 64:ff9b::10.0.0.0/104, 64:ff9b::192.168.0.0/112, etc) will be dropped.  Use a translation prefix within your organization's IPv6 address space instead of 64:ff9b::/96 if you need your IPv6 hosts to communicate with private IPv4 addresses.
Dynamic pool: 192.168.255.0/24
----

Stop the manually started instance with `Ctrl-C`.

.Enable the service explicitly on Ubuntu 18.04 and earlier
[CAUTION]
====
On Ubuntu 18.04 (and earlier), you have to explicitly enable the service. Edit `/etc/default/tayga`. Set `RUN` to `yes`:

.Change in /etc/default/tayga
----
# Change this to "yes" to enable tayga
RUN="yes"
----
====

Launch the service with `systemctl start tayga`. After that, `systemctl status tayga` should report the Active state `active (running)`; the log lines in the status output should end with

----
... systemd[1]: Started LSB: userspace NAT64.
----

.Forgot to enable the service on Ubuntu 18.04 and earlier?
NOTE: If the Active state is `active (exited)` and the protocol says something about `set RUN to yes`, you have forgotten to enable the RUN option in `/etc/default/tayga`. Correct it as described above and issue `systemctl stop tayga` and `systemctl start tayga`.

==== Tayga and firewalls

As described above, Tayga uses the Linux network stack for the IPv4 source NAT step.
For this, it adds a routing rule into the kernel.
You can see it using e.g. `iptables`:

.Tayga NAT routing table entry
----
# iptables -t nat -L
[...]
Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination
[...]
MASQUERADE  all  --  192.168.255.0/24     anywhere
----

It is important that this rule exists, otherwise NAT64 will not work!
Keep this in mind especially if you install a firewall on the physical host.
If that firewall overwrites the complete routing rules set, it will also drop this rule and render Tayga unfunctional.
We cover below how to integrate Tayga <<sec-firewall,with the Shorewall firewall>>.

=== DNS64 with bind

In the last chapter, we have assumed that the IPv6-only system maps IPv4-only targets on a makeshift IPv6 address.
The question remains how it is tricked into doing this.
We solve this problem now.

==== The concept of DNS64

NAT64 is usually used together with a "DNS64" name server. This is a specially configured name server. If a client asks it for an IPv6 name resolution, i.e. an `AAAA` name service record, and there is only an IPv4 `A` record for the requested name, the DNS64 name server "mocks up" an `AAAA` record munging the IPv4 address and a "well-known prefix" to a synthetical IPv6 address. This address - surprise, surprise - points directly to a nicely prepared NAT64 server so that the IPv6 system talks to an IPv4 system transparently hidden behind the NAT64 proxy.

.How DNS64 and NAT64 play together
[plantuml,format="svg",align="center"]
....
@startuml
hide footbox
participant "IPv6-only client" as v
participant "DNS64 server" as e
participant "DNS server" as d
participant "NAT64 server" as n
participant "IPv6-capable server\nwww.example.com" as s6
participant "IPv4-only server\nwww.example.org" as s

== Connect from IPv6 to IPv6 ==

v -> e : Get IP of "www.example.com"
e -> d : Get IP of "www.example.com"
s6 --> d : Address is "2001:1:2:3::5678"
d -> e : Address is "2001:1:2:3::5678"
e -> v : Address is "2001:1:2:3::5678"
v -> s6 : Connect to "2001:1:2:3::5678"
s6 -> v : Return data

== Connect from IPv6 to IPv4 via DNS64/NAT64 ==

v -> e : Get IP of "www.example.org"
e -> d : Get IP of "www.example.org"
s --> d : Address is "1.2.3.4"
d -> e : Address is "1.2.3.4"
e -> v : Address is "64:ff9b::102:304"
v -> n : Connect to "64:ff9b::102:304"
n -> s : Connect to "1.2.3.4"
s -> n : Return data
n -> v : Return data
@enduml
....

==== Installing bind with DNS64

We setup the DNS64 server using a classic bind DNS server. Modern versions include DNS64, it only has to be activated. Start the install with the usual `apt install bind9`.

Our bind is a forwarding-only server only for our own virtual machines. On Debian-derived systems, the bind options needed for this setup are located in `/etc/bind/named.conf.options`. Edit that file and enter the following entries:

.Options for bind in /etc/bind/named.conf.options
----
options {
        directory "/var/cache/bind";

        forwarders {
                2a01:4f8:0:1::add:1010;  # Hetzner name servers
                2a01:4f8:0:1::add:9999;
                2a01:4f8:0:1::add:9898;
        };

        dnssec-validation auto;

        auth-nxdomain no;    # conform to RFC1035
        listen-on {};
        listen-on-v6 {
                <IPv6 network assigned by provider>::/64;
        };
        allow-query { localnets; };
        dns64 64:ff9b::/96 {
                clients { any; };
        };
};
----

The actual important definition is the `dns64` section at the bottom of the `options` definitions. It enables the DNS64 mode of bind and defines the IPv6 address range into which the addresses should be converted.

It also important to define `listen-on {};` to disable listening on the IPv4 port altogether - we do not need it. Restricting `allow-query` to the `localnets` is also important to prevent the server from becoming an open DNS relay. We only need it for our internal network.

The `forwarders` section defines the name servers this bind will ask if it does not know the answer itself - which is almost always the case. I put Hetzner's server names here. Of course, you must either use the DNS of your hoster or provider or a free and open server like Google's public DNS at `2001:4860:4860::8888` and `2001:4860:4860::8844`.

.Check the networks twice
CAUTION: Check the network in `listen-on-v6` and also check the `forwarders`. You whole IP address resolution will not work if one of these is wrong.

Restart the daemon and check that it is enabled and running:

----
systemctl restart bind9
systemctl status bind9
----

After these steps, you have a working DNS64 server which you can use for all you virtual machines on the system.
You can test that it really answers with DNS64-changed entries by querying something which _does not have_ an IPv6 address:

.Obtaining AAAA record for a server which does not have one by DNS64
----
root@physical:~# host github.com  # Query using external default DNS server
github.com has address 140.82.118.3
github.com mail is handled by [...]

root@physical:~# host github.com 2a01:4f8:1:2:3:4:5:6  # Give IPv6 address of local server
[...]
github.com has address 140.82.118.3
github.com has IPv6 address 64:ff9b::8c52:7603
github.com mail is handled by [...]
----

Note how the DNS server running on the physical host returns the _additional_ IPv6 address with `64:ff9b` prefix. To be sure that the local server is really addressed, give its IPv6 address as additional parameter to the `host` command as shown above.

.Using an external DNS64 server
TIP: So far, the name server is only used for DNS64. You can also use the Google servers `2001:4860:4860::6464` and `2001:4860:4860::64` (yes, these are _other_ servers than the public DNS servers mentioned above) offering this service. Their replies are compatible with our NAT64 setup. However, having an own server reduces external dependencies and allows for additional services lateron.

[[sec-dns64-acl]]
==== Restricting DNS64 to certain virtual machines

You can restrict DNS64 service to certain of the virtual machines on the host.
This might be needed as a machine should explicitly _not_ connect to IPv4 servers
or because it has <<sec-add-ipv4,its own IPv4 address>> and should it to connect to the IPv4 internet instead of NAT64.

DNS64 access restriction is done via bind9's access control lists.
Just define an access control list for the DNS64 service and refer to it in the service configuration:

.Access control list for the DNS64 service in /etc/bind/named.conf.options
----
acl dns64clients {
   # address specification
};

options {
        [...]
        dns64 64:ff9b::/96 {
                clients { dns64clients; };  # Refer to the ACL defined above
        };
};
----

There are two ways to specify the servers to allow DNS64 access:

. You can simply specify the IPv6 addresses of all virtual machines which are _allowed_ to use DNS64:
+
.DNS64 ACL with a positive host list
----
acl dns64clients {
   2a01:4f8:1:2:a:bc:345:9;
   2a01:4f8:1:2:a:bc:678:e;
   2a01:4f8:1:2:a:bc:432:7;
   [...]
};
----
+
You _might_ work with net definitions (e.g. `2a01:4f8:1:2:a:bc::/96;`), but normally it does not really make any sense.
The IPv6 addresses of your virtual machines will be derived from the MAC addresses of their (virtual) network cards
and those are assigned randomly when the virtual machine is created.
So, just stick with the actual, full IP adresses here.

. You can also define the control list the other way around and specify those virtual hosts which should _not_ use DNS64:
+
.DNS64 ACL with a negative host list
----
acl dns64clients {
   !2a01:4f8:1:2:a:bc:567:d;
   !2a01:4f8:1:2:a:bc:901:3;
   !2a01:4f8:1:2:a:bc:864:b;
   [...]
   any;  # Grant access for all others!
};
----
+
This option is better if DNS64 is the norm in your setup and you only want to exclude a small number of specific servers.
+
Note that the final entry in your list _must_ be `any;` if you work with negative host specifications - otherwise, no DNS64 service is granted for anyone!


=== Router advertisement with radvd

With NAT64 and DNS64 in place, we're almost ready to serve virtual machines on the host.
The last missing bit is the network configuration.

Of course, you could configure your virtual hosts' network manually.
However, IPv6 offers very nice auto-configuration mechanisms - and they are not difficult to install.
The key component is the "router advertisement daemon".
It's more or less the IPv6-version of the notorious DHCP service used in IPv4 setups to centralize the IP address management.

For this service, we use the `radvd` router advertisement daemon on the bridge device so that our virtual machines get their network setup automatically by reading IPv6 router advertisements.
Install `radvd` and also `radvdump` for testing through the usual Debian/Ubuntu `apt install radvd radvdump`.

Then, create the configuration file `/etc/radvd.conf`. It should contain the following definitions:

.Configuration in /etc/radvd.conf
----
interface br0 {
        AdvSendAdvert on;
        AdvManagedFlag off;
        AdvOtherConfigFlag off;
        AdvDefaultPreference high;
        prefix <IPv6 network assigned by provider>::/64 {
                AdvOnLink on;
                AdvAutonomous on;
                AdvRouterAddr on;
                AdvValidLifetime infinity;
        };
        RDNSS <IPv6 address of the physical host> {};
        route 64:ff9b::/96 {
                AdvRouteLifetime infinity;
        };
};
----

The `route` section advertises that _this_ system routes the `64:ff9b::` network. Only with this definition the virtual servers know where to send the packets for the emulated IPv6 addresses for the IPv4-only servers to.

IPv6 route advertisement is prepared for dynamically changing routes.
In our setup, however, all routes are static.
Therefore, prefix and route advertisements are announced with "infinite" lifetime.

.Use Googles DNS64 servers
[TIP]
====
If you opted for the Google DNS64 servers to do the job, write instead

----
        RDNSS 2001:4860:4860::6464 2001:4860:4860::64 {
                AdvRouteLifetime infinity;
        };
----

This announcement can also have inifinite lifetime. Even if Google changed their server addresses, the definition here stays static.
====

A `radvd` configuration must always be read as advertisement of the machine serving it.
So, you do not write something like "service X is on machine Y" but "_This_ machine offers X".

Having this in mind, the configuration advertises all three network settings needed by the virtual machines:

. The `prefix` section defines that _this_ host announces itself as router (`AdvRouterAddr`) to the given network and allows the machines to use SLAAC for generating their own IPv6 address (`AdvAutonomous`).
. The RDNSS section declares _this_ machine to be the DNS resolver for the virtual machines.
. The `route` section adds the static route for NAT64 IP addresses to _this_ machine.

Start `radvd` and make it a permanent service (coming up automatically after reboot) using

.Commands to activate radvd service
----
systemctl start radvd
systemctl enable radvd
----

If you start `radvdump` soon after starting radvd, you will see the announcements sent by `radvd` in irregular intervals. It should contain the network router, the DNS server and the NAT64 route. Note that radvd turns to rather long intervals between the advertisements after some time if noone is listening.

.Spurious auto-configured routes on br0
[NOTE]
====
After `radvd` is up and running, check the physical host's bridge interface with `ip a show dev br0`. If you find something like

----
    inet6 2a01:4f8:1:2345:abc:4680:1:22/64 scope global dynamic mngtmpaddr noprefixroute
       valid_lft 85234sec preferred_lft 14943sec
----

your bridge is responding to the network announcements. Go back to the network configuration above and add `accept-ra: false` for Netplan or `IPv6AcceptRA=no` for systemd-networkd. On your bridge, all routes must be static (i.e. no `dynamic` modifier) and valid and preferred forever:

----
    inet6 2a01:4f8:1:2345:abc:4680:1:22/64 scope global
       valid_lft forever preferred_lft forever
----
====

If you ever change the configuration, restart `radvd` and check its output with `radvdump`. It should contain both the DNS server and the NAT64 route.

.The nasty Hetzner pitfall
CAUTION: In https://wiki.hetzner.de/index.php/Zusaetzliche_IP-Adressen/en[their own documentation], Hetzner also describes how to setup `radvd`. For the DNS servers, however, they use IPv6 example addresses from the `2001:db8` realm. It took me three days and severe doubts on Hetzner's IPv6 setup to find out, that my only mistake was to copy these wrong IP addresses for the DNS server into the configuration. Don't make the same mistake...

You have now prepared everything for the IPv6-only virtual machines to come: They get their network configuration through the centrally administrated `radvd`. The advertised setup includes a name server with DNS64 an a NAT64 route to access IPv4-only systems.

.About non-virtual network setups
NOTE: So far, this document describes how to setup a root server with virtual machines. Especially NAT64/DNS64 is completely independent of that. If you administrate a (real) computer network and want to lay ground for IPv6-only machines in that, do exactly the same with your physical machines: Install Tayga and the DNS64-capable Bind9 on router behind which the IPv6-only systems reside. This might be the "firewall" of classical setups. Then, your actual computers play the role of the virtual machines in this guide.


== Install KVM and finish the setup

We're now ready for the final steps! Our network is configured far enough so that we really can start installing virtual machines on our system. For this, we of course need KVM.


=== Install `libvirtd` and its environment

For Ubuntu, I followed the first steps of https://www.linuxtechi.com/install-configure-kvm-ubuntu-18-04-server/[this guide]. On Ubuntu 22.04 the installation command is

.Command to install KVM on Ubuntu 22.04
----
apt install bridge-utils libguestfs-tools libosinfo-bin libvirt-clients libvirt-daemon-system libvirt-daemon virtinst qemu qemu-system-x86
----

On Ubuntu 20.04 or 18.04 the list of packages is slightly different

.Command to install KVM on Ubuntu 20.04
----
apt install bridge-utils libguestfs-tools libosinfo-bin libvirt-daemon libvirt-daemon-system qemu-kvm qemu-system virtinst virt-top
----

.Command to install KVM on Ubuntu 18.04
----
apt install bridge-utils libvirt-bin qemu qemu-kvm
----

This will install a rather large number of new packages on your host. Finally, it will be capable to serve virtual machines.


=== Load the virtual network module

Next step is to load the `vhost_net` module into the kernel and make it available permanently. This increases the efficiency of networking for the virtual machines as more work can be done in kernel context and data can be copied less often within the system. Issue two commands:

----
modprobe vhost_net
echo "vhost_net" >> /etc/modules
----

The `libvirtd` daemon should already be up and running at this point. If this is for any reason not the case, start and enable it with the usual `systemctl` commands or whatever the init system of your host server requires to do this.

.Do NOT install dnsmasq on Ubuntu 20.04
[WARNING]
====
If you look into the start messages with `systemctl status libvirtd` on Ubuntu 20.04, you might see a message `Cannot check dnsmasq binary /usr/sbin/dnsmasq: No such file or directory`. *Do not install the dnsmasq package!* The message is misleading and gone with the next restart. If you install dnsmasq, it will fight with bind on the port and your DNS64 service will become unreliable!

Ubuntu 22.04 does not seem to have this quirk.

Note that even if you do not install dnsmasq, you will have a `dnsmasq` process running on the system. This is ok! This program comes from the `dnsmasq-base` package and runs _aside_ of bind without interfering with it.
====


=== Create a non-root user on the system.

To simplify installation and administration of your virtual machines, you should add a "normal" user to the system and allow that user to administrate the virtual machines.

* Create the user with `adduser --disabled-password --gecos "<user name>" <login>`.

* Add the user to the `libvirt` group with `usermod -a -G libvirt <login>.`.

* Put your ssh public key into `.ssh/authorized_keys` of the user.


You should perform a final reboot after these steps to be sure that everything works together correctly and comes up again after a reboot.

Well, that's it! Our system can get its first virtual machine!
